{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danieal3223/REPO-1-/blob/main/(Part_1)_NLP_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmvFilQfrd2M"
      },
      "source": [
        "# <font color=\"maroon\"> NLP Toolkits and Preprocessing Techniques </font>\n",
        "Python libraries for natural language processing\n",
        "1. Converting text to a meaningful format for analysis\n",
        "2. Preprocessing and cleaning text\n",
        "\n",
        "Open-Source Libraries<br>\n",
        "1. <font color=\"red\">NLTK<br> </font>\n",
        "2. <font color=\"red\">TextBlob<br></font>\n",
        "3. SpaCy<br>\n",
        "4. GenSim<br>\n",
        "\n",
        "Cloud-Based NLP Services<br>\n",
        "1. IBM Watson<br>\n",
        "2. Google Cloud Natural Language API\n",
        "3. Amazon Comprehend\n",
        "4. Microsoft Azure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaWPMOOxrd2T"
      },
      "source": [
        "## How to Install NLTK?\n",
        "\n",
        "### Method (i) Command Line\n",
        "pip install nltk<br>\n",
        "import nltk<br>\n",
        "nltk.download()\n",
        "\n",
        "### Method (ii) Anaconda Navigator (Environment)\n",
        "![Installation of NLTK library](NLTK.png)\n",
        "\n",
        "### Method (iii) Download Package and Place into Site-package directory\n",
        "Install nltk toolkit from https://sourceforge.net/projects/nltk/<br>\n",
        "![Installation of NLTK library](nltk_package.png)\n",
        "<br>Locate the package into site-package directory <br>\n",
        "(to find the path:<br> import site <br>site.getsitepackages())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AfqK8WmIrd2U",
        "outputId": "240cf051-3d11-4d0b-eabb-4752ec71b73a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/lib/python3.11/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/lib/python3.11/dist-packages']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import site\n",
        "site.getsitepackages()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySR0BFTirn-H"
      },
      "source": [
        "# Method 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWANqwFjrn-H",
        "outputId": "cc86561b-8d33-4676-817f-821e68d8cd2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk           #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VDPu2TKnrn-H"
      },
      "outputs": [],
      "source": [
        "import nltk #complete this\n",
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.data.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1fD-GTDt1s9",
        "outputId": "d343d3cd-596d-47ef-bfeb-e829cd0424ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9fuAB41rd2W"
      },
      "source": [
        "## Sample Text Data\n",
        "\n",
        "Consider this sentence:\n",
        "<br>**Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers) from the\n",
        "store. Should I pick up some black-eyed peas as well?**\n",
        "\n",
        "Text data is messy and unstructured. To analyze this data, we need to preprocess the text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gctL6qcrd2Y"
      },
      "source": [
        "![](https://i.imgur.com/pt5p6Hb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJGrFa0qrd2Y"
      },
      "source": [
        "# Code: Tokenization (Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s_xmMSXArd2Y",
        "outputId": "fa397fd8-61fe-406a-fcef-fc0130bc55c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'Mr.', 'Smith', '!', 'I', 'am', 'going', 'to', 'buy', 'some', 'vegetables', '(', '3', 'tomatoes', 'and', '3', 'cucumbers', ')', 'from', 'the', 'store', '.', 'Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well', '?']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize #complete this\n",
        "\n",
        "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
        "from the store. Should I pick up some black-eyed peas as well?'''\n",
        "\n",
        "print(word_tokenize(my_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbzU3hrZrd2Z"
      },
      "source": [
        "# Code: Tokenization (Sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vEFjou9Mrd2a",
        "outputId": "f6851e95-98f5-4cc3-9f7d-2106db7e12e1",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi Mr. Smith!', 'I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\\nfrom the store.', 'Should I pick up some black-eyed peas as well?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize #complete this\n",
        "\n",
        "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
        "from the store. Should I pick up some black-eyed peas as well?'''\n",
        "\n",
        "print(sent_tokenize(my_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOhJjUDlrd2e"
      },
      "source": [
        "![](https://i.imgur.com/3L6x92C.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr8o5eNmrd2e"
      },
      "source": [
        "# Code: Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IgqeFhzerd2e",
        "outputId": "5c68b1ce-cbdf-4eb6-9d55-57817e1b09b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Mr Smith I am going to buy some vegetables 3 tomatoes and 3 cucumbers\\nfrom the store Should I pick up some blackeyed peas as well'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import re #complete this\n",
        "import string\n",
        "\n",
        "#Replace punctuations with a white space\n",
        "s = re.sub('[^\\w\\s]','', my_text)             #complete this\n",
        "s\n",
        "\n",
        "#OR\n",
        "# clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)  # string.punctuation is a string defined in the string module of Python. It contains all the punctuation characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~`.\n",
        "# clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwv4BKQzrd2f"
      },
      "source": [
        "# Code: Make All Text Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "evdfIo5Ird2g",
        "outputId": "6a7e8562-5d7f-4c06-a35d-bdb161e1ea00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI MR SMITH I AM GOING TO BUY SOME VEGETABLES 3 TOMATOES AND 3 CUCUMBERS\\nFROM THE STORE SHOULD I PICK UP SOME BLACKEYED PEAS AS WELL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "clean_text = s.upper()#complete this\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DEi4_gYrd2h"
      },
      "source": [
        "# Code: Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AAuSuRNBrd2h",
        "outputId": "8beac6d1-686e-47a2-c5e2-23af8f8fb404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI MR SMITH I AM GOING TO BUY SOME VEGETABLES  TOMATOES AND  CUCUMBERS\\nFROM THE STORE SHOULD I PICK UP SOME BLACKEYED PEAS AS WELL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Removes all words containing digits\n",
        "clean_text = re.sub('\\d', '', clean_text)  #complete this\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAyxd4Kgrd2i"
      },
      "source": [
        "# <font color='blue'>Preprocessing: Stop Words</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI9336Hqrd2i"
      },
      "source": [
        "![](https://i.imgur.com/T5RJXrX.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg_vlZdErd4Z"
      },
      "source": [
        "# Code: Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zwAJ5PK1rd4Z",
        "outputId": "7108eb00-b170-4fe4-b4d3-251a738495d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from nltk.corpus import stopwords #complete this\n",
        "nltk.download('stopwords')\n",
        "set(stopwords.words('english'))              #complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWIOjUCrd4a"
      },
      "source": [
        "# Code: Remove Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKfQc-qfrd4a"
      },
      "source": [
        "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8fAaaw_Zrd4b",
        "outputId": "58213bbe-0c94-4c22-8dc4-b3fc45c60828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 13 stored elements and shape (1, 13)>\n",
            "  Coords\tValues\n",
            "  (0, 5)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 7)\t1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   black  buy  cucumbers  eyed  going  hi  mr  peas  pick  smith  store  \\\n",
              "0      1    1          1     1      1   1   1     1     1      1      1   \n",
              "\n",
              "   tomatoes  vegetables  \n",
              "0         1           1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2e8884e-03e1-4517-bf0a-5a0fc33c94dd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>black</th>\n",
              "      <th>buy</th>\n",
              "      <th>cucumbers</th>\n",
              "      <th>eyed</th>\n",
              "      <th>going</th>\n",
              "      <th>hi</th>\n",
              "      <th>mr</th>\n",
              "      <th>peas</th>\n",
              "      <th>pick</th>\n",
              "      <th>smith</th>\n",
              "      <th>store</th>\n",
              "      <th>tomatoes</th>\n",
              "      <th>vegetables</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2e8884e-03e1-4517-bf0a-5a0fc33c94dd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e2e8884e-03e1-4517-bf0a-5a0fc33c94dd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e2e8884e-03e1-4517-bf0a-5a0fc33c94dd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# self learn numpy: https://www\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"black\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cucumbers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eyed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"going\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"peas\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pick\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smith\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"store\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tomatoes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vegetables\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer#complete this\n",
        "import pandas as pd\n",
        "\n",
        "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables \\\n",
        "(3 tomatoes and 3 cucumbers from the store. Should I pick up some black-eyed peas as well?\"]\n",
        "\n",
        "# Incorporate stop words when creating the count vectorizer\n",
        "cv = CountVectorizer(stop_words='english') #complete this\n",
        "X = cv.fit_transform(my_text)                                       #complete this\n",
        "print (X)\n",
        "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())\n",
        "\n",
        "# Reference: https://www.geeksforgeeks.org/difference-between-pandas-vs-numpy/\n",
        "# self learn pandas: https://www.w3schools.com/python/pandas/pandas_intro.asp\n",
        "# self learn numpy: https://www.w3schools.com/python/numpy/numpy_intro.asp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPbENbQrd4b"
      },
      "source": [
        "The process of using CountVectorizer.fit_transform involves the following steps:\n",
        "\n",
        "(1)Tokenization: The text documents are first tokenized, breaking them into individual words or tokens.\n",
        "\n",
        "(2)Vocabulary Building (fit): CountVectorizer builds a vocabulary, which is a dictionary mapping each unique word (or token) in the documents to an integer index.\n",
        "\n",
        "(3)Counting (transform): It then counts the occurrences of each word in each document and stores these counts in a sparse matrix, where rows represent documents, and columns represent the vocabulary words. Each element of the matrix represents the frequency of the corresponding word in the respective document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OMyEkOQrd4c"
      },
      "source": [
        "![](https://i.imgur.com/9qllh8j.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlkukpcRrd4c"
      },
      "source": [
        "# Code: Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yUzdKeTard4c",
        "outputId": "b3965e85-129b-4c8c-e447-b774b0ddd70f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive:driv\n",
            "drives:driv\n",
            "driver:driv\n",
            "drivers:driv\n",
            "driven:driv\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer#complete this\n",
        "stemmer = LancasterStemmer()#complete this\n",
        "\n",
        "# Try some stems\n",
        "print('drive:{}'.format(stemmer.stem('drive')))\n",
        "print('drives:{}'.format(stemmer.stem('drives')))\n",
        "print('driver:{}'.format(stemmer.stem('driver')))\n",
        "print('drivers:{}'.format(stemmer.stem('drivers')))\n",
        "print('driven:{}'.format(stemmer.stem('driven')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer#complete this\n",
        "stemmer = PorterStemmer()#complete this\n",
        "\n",
        "# Try some stems\n",
        "print('drive:{}'.format(stemmer.stem('drive')))\n",
        "print('drives:{}'.format(stemmer.stem('drives')))\n",
        "print('driver:{}'.format(stemmer.stem('driver')))\n",
        "print('drivers:{}'.format(stemmer.stem('drivers')))\n",
        "print('driven:{}'.format(stemmer.stem('driven')))"
      ],
      "metadata": {
        "id": "9UTl7zKczVqX",
        "outputId": "bdb29d2c-d87d-4831-a5c9-fa8af909ee80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive:drive\n",
            "drives:drive\n",
            "driver:driver\n",
            "drivers:driver\n",
            "driven:driven\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvJnHx7Krd4c"
      },
      "source": [
        "# Code: Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zMCMR9c-rd4d",
        "outputId": "e5b50628-c0f0-4222-edeb-c71a4cb86763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n",
            "running\n",
            "fly\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer#complete this  # Reference: https://www.nltk.org/api/nltk.stem.wordnet.html\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "input_str = \"been had done languages cities mice running flies\"\n",
        "input_str = word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkUcOHgBrn-L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kvAH-Lrd4d"
      },
      "source": [
        "![](https://i.imgur.com/8edVsCR.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHDjq6x_rd4e"
      },
      "source": [
        "# Code: Parts of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU1uQFs1rd4e",
        "outputId": "b963f0dc-2e25-42df-890e-999b147b0370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: [('James', 'NNP'), ('Smith', 'NNP'), ('lives', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n",
            "Sentence 2: [('James', 'NNP'), ('Smith', 'NNP'), ('is', 'VBZ'), ('having', 'VBG'), ('a', 'DT'), ('live', 'JJ'), ('band', 'NN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tag import pos_tag#complete this\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "my_text2 = \"James Smith is having a live band in the United States.\"\n",
        "tokens = pos_tag(word_tokenize(my_text))\n",
        "tokens2 = pos_tag(word_tokenize(my_text2))\n",
        "print(\"Sentence 1:\",tokens)\n",
        "print(\"Sentence 2:\",tokens2)\n",
        "\n",
        "#Reference:https://pythonspot.com/nltk-speech-tagging/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vfPiB0Erd4f"
      },
      "source": [
        "![POS](nltk-speech-codes.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfpmNQBHrd4f"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "-clqK5cCrd4f",
        "outputId": "7076c31e-f786-4717-c9af-dfa71fb2587c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-2322008140.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this labels each word as a part of speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this extracts entities from the list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ],
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "from nltk.chunk import ne_chunk #complete this\n",
        "\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
        "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
        "entities.draw()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk#complete this\n",
        "\n",
        "#download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "\n",
        "tokens = word_tokenize(my_text) # this labels each word as a part of speech\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "entities = ne_chunk(tagged_tokens)\n",
        "\n",
        "entities.draw()"
      ],
      "metadata": {
        "id": "G9EnEKuH1-WD",
        "outputId": "ca46cf88-5215-4e41-897a-063a6b6c89fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-3247453708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjiixNgFrd4f"
      },
      "source": [
        "# <font color=\"blue\"> Prepocessing: Compound Term Extraction </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsIaKOxrd4h"
      },
      "source": [
        "![](https://i.imgur.com/q1WuWai.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4KwWt67rd4h"
      },
      "source": [
        "# Code: Compound Term Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pn3S4RwHrd4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c241250-0acf-4c48-d4eb-267b4d4414bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['You_all', 'are', 'the', 'greatest', 'students', 'of_all_time', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import MWETokenizer #complete this\n",
        "\n",
        "my_text = \"You all are the greatest students of all time.\"\n",
        "\n",
        "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
        "\n",
        "mwe_tokens\n",
        "\n",
        "# New York City, take into account, make use of, high probability, kick the bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44VAWC7orn-O"
      },
      "source": [
        "# Lambda Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hbPyyQ_7rd4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b1de05-4aae-41b6-9664-e18deacdd114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81, 9, 16, 10000, 4, 1]\n"
          ]
        }
      ],
      "source": [
        "# Basic example, https://www.w3schools.com/python/python_lambda.asp\n",
        "square_me=lambda x: x*x\n",
        "\n",
        "my_numbers=[9, 3, 4, 100, 2, 1]\n",
        "my_numbers_squared = list(map(square_me, my_numbers)) #map = applies a function to all the items in an input_list\n",
        "                                                      #map(function, iterable)\n",
        "print(my_numbers_squared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1rrNgqErd4u"
      },
      "source": [
        "# <font color=red>Preprocessing Exercise </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G37XzEBLrd4u"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We will be using review data from Kaggle to practice preprocessing text data. The dataset contains user reviews for many products, but today we'll be focusing on the product in the dataset that had the most reviews - an oatmeal cookie.\n",
        "\n",
        "The following code will help you load in the data. If this is your first time using nltk, you'll to need to pip install it first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UN4M0O6krd4v"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download() <-- Run this if it's your first time using nltk to download all of the datasets and models\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "-7wE2LEHrd4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "28a83f2a-7aab-4fa7-dcb6-5cf9351dd00b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          user_id  stars                                            reviews\n",
              "0  A368Z46FIKHSEZ      5  I love these cookies!  Not only are they healt...\n",
              "1  A1JAPP1CXRG57A      5  Quaker Soft Baked Oatmeal Cookies with raisins...\n",
              "2  A2Z9JNXPIEL2B9      5  I am usually not a huge fan of oatmeal cookies...\n",
              "3  A31CYJQO3FL586      5  I participated in a product review that includ...\n",
              "4  A2KXQ2EKFF3K2G      5  My kids loved these. I was very pleased to giv..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac0d417b-e6d6-4455-9ba3-d907c7869a63\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A368Z46FIKHSEZ</td>\n",
              "      <td>5</td>\n",
              "      <td>I love these cookies!  Not only are they healt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A1JAPP1CXRG57A</td>\n",
              "      <td>5</td>\n",
              "      <td>Quaker Soft Baked Oatmeal Cookies with raisins...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A2Z9JNXPIEL2B9</td>\n",
              "      <td>5</td>\n",
              "      <td>I am usually not a huge fan of oatmeal cookies...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A31CYJQO3FL586</td>\n",
              "      <td>5</td>\n",
              "      <td>I participated in a product review that includ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A2KXQ2EKFF3K2G</td>\n",
              "      <td>5</td>\n",
              "      <td>My kids loved these. I was very pleased to giv...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac0d417b-e6d6-4455-9ba3-d907c7869a63')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac0d417b-e6d6-4455-9ba3-d907c7869a63 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac0d417b-e6d6-4455-9ba3-d907c7869a63');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2891920b-58e0-43be-a13d-1a0423e68368\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2891920b-58e0-43be-a13d-1a0423e68368')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2891920b-58e0-43be-a13d-1a0423e68368 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 913,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 913,\n        \"samples\": [\n          \"A1RO932ZM1ZYNC\",\n          \"A27TITC8ON9CMZ\",\n          \"A1G2F4L08EMS24\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reviews\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 910,\n        \"samples\": [\n          \"Quaker's Soft Baked Oatmeal Cookies with Raisins are OK, but they left me wanting something better.  I thought they tasted good, but I found them to be a little on the dry side.  A lot of packaged cookies have an odd chemical aftertaste, but I didn't really notice much with these.<br /><br />However, I would like to see better choices on ingredients.  First, I don't understand the need for both bleached and unbleached flour.  Quaker should just stick with unbleached.  Second, I accept sugar as an ingredient.  These are cookies.  Cookies are sweet, and I expect sugar.  Yet why do they also have to use fructose, invert sugar syrup, corn syrup, and polydextrose?  These also contain molasses, which are a natural sweetener.  Finally, these have both corn starch and modified corn starch.  Again, why?  You may find other ingredients that you would question, but these seemed like the glaring issues to me.  In the end, I think this is too much to overcome when you combine it with a relatively high percentage (60 out of 170) of calories from fat.  In short, I cannot recommend these cookies.\",\n          \"Cookies are delicious and very soft, i expected a much different taste but to my surpise they were actually great. I got a sample so i will be purchasing these for my family soon.\",\n          \"Quaker Soft Baked Oatmeal Cookies are so good! They taste like a homemade cookie! If you want that right out of the oven taste try heating up the cookie for about ten seconds. It will be warm and soft!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "df = pd.read_csv('cookie_reviews.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-TQNFEqrd4w"
      },
      "source": [
        "**Question 1:**\n",
        "\n",
        "Determine how many reviews there are in total.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VNVnbS1rd4w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2SHURS_rd4x"
      },
      "source": [
        "**Question 2:**\n",
        "    \n",
        "Determine the percentage of 1, 2, 3, 4 and 5 star reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrh7DCk4rd4y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHZ7MVt-rd4y"
      },
      "source": [
        "**Question 3:**\n",
        "\n",
        "(a) Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beCkKopYrd4z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHXKLaHfrn-P"
      },
      "source": [
        "1. df['reviews'] refers to the 'reviews' column in your DataFrame df\n",
        "2. .apply(lambda x: ...) is used to apply a function (defined by the lambda function) along the axis of the DataFrame.\n",
        "3. lambda x: ' '.join([word for word in x.split() if word not in (stop)]) is a lambda function that:\n",
        "   <br>a. Splits each review x into a list of words (x.split()).\n",
        "   <br>b. Iterates through each word in this list (for word in x.split()).\n",
        "   <br>c. Checks if each word is not in the stop list (i.e., if it's not a stopword).\n",
        "   <br>d. If the word is not a stopword, it includes it in the list comprehension ([word for word in x.split() if word not in (stop)]).\n",
        "   <br>e. Joins these words back into a single string with spaces separating them (' '.join(...))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr4R-PKWrd4z"
      },
      "source": [
        "(b) Change to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LopdQ216rd40"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00e5zCM5rd40"
      },
      "source": [
        "(b) Perform stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h_Oeeuwrd41"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNEsf7zDrn-Q"
      },
      "source": [
        "1. Constructs a new list (documents) by iterating over each element (x) in the list l_case.\n",
        "2. For each document i in l_case, the inner list comprehension splits i into words using i.split(\" \").\n",
        "3. It then applies stemming to each word using sno.stem(word), where sno is an object or function that performs stemming.\n",
        "4. The outer comprehension gathers these lists of stemmed words (one list per document) and constructs a new list (documents) where each element corresponds to a document from l_case, but with each word stemmed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKTiJ341rd43"
      },
      "source": [
        "# TextBlob\n",
        "\n",
        "### Another toolkit other than NLTK\n",
        "\n",
        "- Wraps around NLTK and makes it easier to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtSKm8Jmrd44"
      },
      "source": [
        "# TextBlob Demo: Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VANPFYO2rn-Q"
      },
      "outputs": [],
      "source": [
        "#pip install textblob  #Install the library before importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sm_gcqLrd44"
      },
      "outputs": [],
      "source": [
        "from textblob import                   #complete this\n",
        "\n",
        "my_text = TextBlob(\"We're moving from NLTK to TextBlob. How fun!\")\n",
        "my_text.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz6kz6o6rd44"
      },
      "source": [
        "# TextBlob Demo: Spell Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7x_TFgdrd45"
      },
      "outputs": [],
      "source": [
        "blob = TextBlob(\"I'm graat at speling.\")\n",
        "print(blob.)                                   #complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvfqZ_ulrd45"
      },
      "source": [
        "<font color=\"blue\">\n",
        "## How does the correct function work?  <br>\n",
        "    \n",
        "- Calculates the Levenshtein distance between the word ‘graat’ and all words in its word list </br>\n",
        "- Of the words with the smallest Levenshtein distance, it outputs the most popular word </br></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNRUkzwyrd46"
      },
      "source": [
        "# TextBlob Demo: Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVr6l2Bkrd46"
      },
      "outputs": [],
      "source": [
        "blob = TextBlob(\"John hits the ball.\")\n",
        "for words, tag in :                           #complete this\n",
        "    print (words, tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRWBpLVfrd46"
      },
      "source": [
        "# TextBlob Demo: Language Detection and Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uimeap4prd47"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"This is a sample text in English.\"\n",
        "blob = TextBlob(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-9hqISZrd47"
      },
      "outputs": [],
      "source": [
        "!pip install                                  #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j9k5_p1rd47"
      },
      "outputs": [],
      "source": [
        "from langdetect import                       #complete this\n",
        "\n",
        "text = \"This is a sample text in English.\"\n",
        "language = detect(text)\n",
        "\n",
        "print(\"Detected Language:\", language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIMWwVwxrn-T"
      },
      "outputs": [],
      "source": [
        "!pip install                        #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wLwTF_trd5g"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "text = \"This is a sample text in English.\"\n",
        "\n",
        "# Detect the language\n",
        "detected_lang = detect(text)\n",
        "\n",
        "# Translate to French\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(text, src=detected_lang, dest='fr').text             #try different language\n",
        "\n",
        "print(\"Detected Language:\", detected_lang)\n",
        "print(\"Translated Text (to French):\", translated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XADY612Arn-T"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxs6mO58rn-T"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using TextBlob to tokenize a given sentence and count the number of tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY5hSejQrn-T"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using TextBlob to perform Parts of Speech (POS) tagging on a given sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doy_8CtErn-T"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using TextBlob to perform spell checking on a given text and suggest corrections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slYJnfVQrn-U"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using langdetect and googletrans to perform trasnlation on a given text from english to chiense\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiushqSmrd41"
      },
      "source": [
        "# <font color=\"maroon\"> Some other functions in NLP: Text Similarity Measures </font>\n",
        "\n",
        "- To measure distance between 2 string\n",
        "\n",
        "Applications\n",
        "- Information retrieval\n",
        "- Text classification\n",
        "- Document clustering\n",
        "- Topic Modeling\n",
        "- Matric decomposition\n",
        "\n",
        "To measure the word similarity, we use **<font color=\"blue\"><a href=\"https://pypi.org/project/python-Levenshtein/\" target=\"_blank\">Levenshtein distance</a></font>**.\n",
        "- Minimum number of operations to get from one word to another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHCcSiv3rd41"
      },
      "source": [
        "![](https://i.imgur.com/FkdJmPi.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bvYNWyGrd42"
      },
      "outputs": [],
      "source": [
        "pip install                   #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MlCBsGsrd42"
      },
      "outputs": [],
      "source": [
        "from Levenshtein import distance as lev\n",
        "lev('party', 'park')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svFS8eNIrd43"
      },
      "outputs": [],
      "source": [
        "#concept behind lev('party', 'park')\n",
        "def levenshtein_distance(s1, s2):\n",
        "    m, n = len(s1), len(s2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
        "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
        "\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "string1 = \"party\"\n",
        "string2 = \"park\"\n",
        "distance =                                        #complete this\n",
        "print(\"Levenshtein distance:\", distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-j8ZtXrn-U"
      },
      "source": [
        "## Let's use the Levenshtein to measure the similarity between 2 sentences:\n",
        "<br>sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "<br>sentence2 = \"A quick brown fox jumps over a lazy dog.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJ8pdusrn-U"
      },
      "outputs": [],
      "source": [
        "from Levenshtein import distance as lev\n",
        "\n",
        "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence2 = \"A quick brown fox jumps over a lazy dog.\"\n",
        "\n",
        "words1 = sentence1.lower().split()\n",
        "words2 = sentence2.lower().split()\n",
        "\n",
        "distance = lev(words1, words2)\n",
        "\n",
        "# Calculate similarity (adjust based on your specific needs)\n",
        "max_length = max(len(words1), len(words2))\n",
        "# print (max_length)\n",
        "similarity = 1 - (distance / max_length)\n",
        "\n",
        "print(\"Levenshtein distance between sentence 1 and sentence 2:\", distance)\n",
        "print(\"Similarity between sentence 1 and sentence 2:\", similarity)\n",
        "\n",
        "# However, it's important to note that Levenshtein distance is typically used for comparing sequences of characters, not entire sentences or phrases.\n",
        "# To measure similarity between sentences where the words are not necessarily in the same sequence,\n",
        "# you need to consider methods that can account for semantic similarity rather than just sequence-based similarity like Levenshtein distance.\n",
        "# Here are a few approaches you can explore: TF-IDF/Word Embeddings (pretrained model like Word2Vec, GloVe, or FastText) and Similarity Metrics (Cosine Similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk2GVCZ-rd5g"
      },
      "source": [
        "# Text Format for Analysis: Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD1MS75vrd5g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus =['This is the first document.', 'This is the second document.', 'And the third one. One is fun.'] #corpus=collection of teks\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus)\n",
        "pd.DataFrame(X.toarray(),columns=cv.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdvU8fEVrd5h"
      },
      "source": [
        "![](https://i.imgur.com/OQDeQlb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv94wmmjrd5h"
      },
      "source": [
        "# Document Similarity: Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYKH_Ntyrd5i"
      },
      "source": [
        "![](https://i.imgur.com/PyirXsy.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLZ-ub-Yrd5j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = ['The weather is hot under the sun',\n",
        "'I make my hot chocolate with milk',\n",
        "'One hot encoding',\n",
        "'I will have a chai latte with milk',\n",
        "'There is a hot sale today']\n",
        "# create the document-term matrix with count vectorizer\n",
        "cv = CountVectorizer(stop_words=\"english\")\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zzehizvrd5l"
      },
      "source": [
        "# Document Similarity: Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwo3s5KIrd5l"
      },
      "outputs": [],
      "source": [
        "# calculate the cosine similarity between all combinations of documents\n",
        "from itertools import combinations\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# list all of the combinations of 5 take 2 as well as the pairs of phrases\n",
        "pairs = list(combinations(range(len(corpus)),2)) #sentence (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), .., (3,4))\n",
        "print(pairs)\n",
        "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
        "print (combos)\n",
        "\n",
        "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
        "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
        "sorted(zip(results, combos), reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C9J5J45rd5m"
      },
      "outputs": [],
      "source": [
        "pairs = list(combinations(range(5),2))\n",
        "pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYV4dhzfrd5m"
      },
      "source": [
        "![](https://i.imgur.com/jrfN6Jj.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS0Stotjrd5m"
      },
      "source": [
        "![](https://i.imgur.com/BI8XP92.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_X6soeOrd5n"
      },
      "source": [
        "![](https://i.imgur.com/3IbfQXT.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaTIJB99rd5n"
      },
      "source": [
        "![](https://i.imgur.com/pnNqzql.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBKAYd0erd5n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "corpus = ['This is the first document.',\n",
        "         'This is the second document.',\n",
        "         'And the third one. One is fun.']\n",
        "# original Count Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "pd.DataFrame(X, columns=cv.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u44cBAImrd5o"
      },
      "outputs": [],
      "source": [
        "# new TF-IDF Vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv_tfidf = TfidfVectorizer()\n",
        "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
        "pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEMme_lQrd5o"
      },
      "source": [
        "![](https://i.imgur.com/xlJibKw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIQbXx_Jrd5p"
      },
      "source": [
        "## Document Similarity: Example with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmQKsUwCrd5p"
      },
      "outputs": [],
      "source": [
        "corpus = ['The weather is hot under the sun',\n",
        "'I make my hot chocolate with milk',\n",
        "'One hot encoding',\n",
        "'I will have a chai latte with milk',\n",
        "'There is a hot sale today']\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# create the document-term matrix with TF-IDF vectorizer\n",
        "cv_tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
        "dt_tfidf = pd.DataFrame(X_tfidf,columns=cv_tfidf.get_feature_names())\n",
        "dt_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV41uOddrd5p"
      },
      "outputs": [],
      "source": [
        "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
        "results_tfidf = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
        "sorted(zip(results_tfidf, combos), reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StpSLAU1rd5q"
      },
      "source": [
        "![](https://i.imgur.com/mj4J60v.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}