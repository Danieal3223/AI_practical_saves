{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danieal3223/REPO-1-/blob/main/(Part_1)_NLP_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmvFilQfrd2M"
      },
      "source": [
        "# <font color=\"maroon\"> NLP Toolkits and Preprocessing Techniques </font>\n",
        "Python libraries for natural language processing\n",
        "1. Converting text to a meaningful format for analysis\n",
        "2. Preprocessing and cleaning text\n",
        "\n",
        "Open-Source Libraries<br>\n",
        "1. <font color=\"red\">NLTK<br> </font>\n",
        "2. <font color=\"red\">TextBlob<br></font>\n",
        "3. SpaCy<br>\n",
        "4. GenSim<br>\n",
        "\n",
        "Cloud-Based NLP Services<br>\n",
        "1. IBM Watson<br>\n",
        "2. Google Cloud Natural Language API\n",
        "3. Amazon Comprehend\n",
        "4. Microsoft Azure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaWPMOOxrd2T"
      },
      "source": [
        "## How to Install NLTK?\n",
        "\n",
        "### Method (i) Command Line\n",
        "pip install nltk<br>\n",
        "import nltk<br>\n",
        "nltk.download()\n",
        "\n",
        "### Method (ii) Anaconda Navigator (Environment)\n",
        "![Installation of NLTK library](NLTK.png)\n",
        "\n",
        "### Method (iii) Download Package and Place into Site-package directory\n",
        "Install nltk toolkit from https://sourceforge.net/projects/nltk/<br>\n",
        "![Installation of NLTK library](nltk_package.png)\n",
        "<br>Locate the package into site-package directory <br>\n",
        "(to find the path:<br> import site <br>site.getsitepackages())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AfqK8WmIrd2U",
        "outputId": "240cf051-3d11-4d0b-eabb-4752ec71b73a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/lib/python3.11/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/lib/python3.11/dist-packages']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import site\n",
        "site.getsitepackages()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySR0BFTirn-H"
      },
      "source": [
        "# Method 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PWANqwFjrn-H",
        "outputId": "cc86561b-8d33-4676-817f-821e68d8cd2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk           #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VDPu2TKnrn-H"
      },
      "outputs": [],
      "source": [
        "import nltk #complete this\n",
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.data.path)"
      ],
      "metadata": {
        "id": "Z1fD-GTDt1s9",
        "outputId": "d343d3cd-596d-47ef-bfeb-e829cd0424ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9fuAB41rd2W"
      },
      "source": [
        "## Sample Text Data\n",
        "\n",
        "Consider this sentence:\n",
        "<br>**Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers) from the\n",
        "store. Should I pick up some black-eyed peas as well?**\n",
        "\n",
        "Text data is messy and unstructured. To analyze this data, we need to preprocess the text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gctL6qcrd2Y"
      },
      "source": [
        "![](https://i.imgur.com/pt5p6Hb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJGrFa0qrd2Y"
      },
      "source": [
        "# Code: Tokenization (Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s_xmMSXArd2Y",
        "outputId": "651ad221-681f-443b-ee0c-bb4411d9fbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-11-1837410625.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-11-1837410625.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from nltk.tokenize import                     #complete this\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import                     #complete this\n",
        "\n",
        "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
        "from the store. Should I pick up some black-eyed peas as well?'''\n",
        "\n",
        "print(word_tokenize(my_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbzU3hrZrd2Z"
      },
      "source": [
        "# Code: Tokenization (Sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEFjou9Mrd2a",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import            #complete this\n",
        "\n",
        "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
        "from the store. Should I pick up some black-eyed peas as well?'''\n",
        "\n",
        "print(sent_tokenize(my_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOhJjUDlrd2e"
      },
      "source": [
        "![](https://i.imgur.com/3L6x92C.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr8o5eNmrd2e"
      },
      "source": [
        "# Code: Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgqeFhzerd2e"
      },
      "outputs": [],
      "source": [
        "import               #complete this\n",
        "import string\n",
        "\n",
        "#Replace punctuations with a white space\n",
        "s = re.sub('','',my_text)             #complete this\n",
        "s\n",
        "\n",
        "#OR\n",
        "# clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)  # string.punctuation is a string defined in the string module of Python. It contains all the punctuation characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~`.\n",
        "# clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwv4BKQzrd2f"
      },
      "source": [
        "# Code: Make All Text Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evdfIo5Ird2g"
      },
      "outputs": [],
      "source": [
        "clean_text =                 #complete this\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DEi4_gYrd2h"
      },
      "source": [
        "# Code: Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAuSuRNBrd2h"
      },
      "outputs": [],
      "source": [
        "# Removes all words containing digits\n",
        "clean_text = re.sub('', '', clean_text)  #complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAyxd4Kgrd2i"
      },
      "source": [
        "# <font color='blue'>Preprocessing: Stop Words</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI9336Hqrd2i"
      },
      "source": [
        "![](https://i.imgur.com/T5RJXrX.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg_vlZdErd4Z"
      },
      "source": [
        "# Code: Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwAJ5PK1rd4Z"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import                #complete this\n",
        "set(stopwords.words(''))              #complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWIOjUCrd4a"
      },
      "source": [
        "# Code: Remove Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKfQc-qfrd4a"
      },
      "source": [
        "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fAaaw_Zrd4b"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import             #complete this\n",
        "import pandas as pd\n",
        "\n",
        "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables \\\n",
        "(3 tomatoes and 3 cucumbers from the store. Should I pick up some black-eyed peas as well?\"]\n",
        "\n",
        "# Incorporate stop words when creating the count vectorizer\n",
        "cv =                                          #complete this\n",
        "X = cv.                                       #complete this\n",
        "print (X)\n",
        "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())\n",
        "\n",
        "# Reference: https://www.geeksforgeeks.org/difference-between-pandas-vs-numpy/\n",
        "# self learn pandas: https://www.w3schools.com/python/pandas/pandas_intro.asp\n",
        "# self learn numpy: https://www.w3schools.com/python/numpy/numpy_intro.asp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPbENbQrd4b"
      },
      "source": [
        "The process of using CountVectorizer.fit_transform involves the following steps:\n",
        "\n",
        "(1)Tokenization: The text documents are first tokenized, breaking them into individual words or tokens.\n",
        "\n",
        "(2)Vocabulary Building (fit): CountVectorizer builds a vocabulary, which is a dictionary mapping each unique word (or token) in the documents to an integer index.\n",
        "\n",
        "(3)Counting (transform): It then counts the occurrences of each word in each document and stores these counts in a sparse matrix, where rows represent documents, and columns represent the vocabulary words. Each element of the matrix represents the frequency of the corresponding word in the respective document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OMyEkOQrd4c"
      },
      "source": [
        "![](https://i.imgur.com/9qllh8j.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlkukpcRrd4c"
      },
      "source": [
        "# Code: Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUzdKeTard4c"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import          #complete this\n",
        "stemmer =                      #complete this\n",
        "\n",
        "# Try some stems\n",
        "print('drive:{}'.format(stemmer.stem('drive')))\n",
        "print('drives:{}'.format(stemmer.stem('drives')))\n",
        "print('driver:{}'.format(stemmer.stem('driver')))\n",
        "print('drivers:{}'.format(stemmer.stem('drivers')))\n",
        "print('driven:{}'.format(stemmer.stem('driven')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvJnHx7Krd4c"
      },
      "source": [
        "# Code: Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMCMR9c-rd4d"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import                       #complete this  # Reference: https://www.nltk.org/api/nltk.stem.wordnet.html\n",
        "from nltk.tokenize import word_tokenize\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "input_str = \"been had done languages cities mice running flies\"\n",
        "input_str = word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkUcOHgBrn-L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kvAH-Lrd4d"
      },
      "source": [
        "![](https://i.imgur.com/8edVsCR.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHDjq6x_rd4e"
      },
      "source": [
        "# Code: Parts of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yU1uQFs1rd4e"
      },
      "outputs": [],
      "source": [
        "from nltk.tag import                                #complete this\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "my_text2 = \"James Smith is having a live band in the United States.\"\n",
        "tokens = pos_tag(word_tokenize(my_text))\n",
        "tokens2 = pos_tag(word_tokenize(my_text2))\n",
        "print(\"Sentence 1:\",tokens)\n",
        "print(\"Sentence 2:\",tokens2)\n",
        "\n",
        "#Reference:https://pythonspot.com/nltk-speech-tagging/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vfPiB0Erd4f"
      },
      "source": [
        "![POS](nltk-speech-codes.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfpmNQBHrd4f"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-clqK5cCrd4f"
      },
      "outputs": [],
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.chunk import                         #complete this\n",
        "\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
        "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
        "entities.draw()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjiixNgFrd4f"
      },
      "source": [
        "# <font color=\"blue\"> Prepocessing: Compound Term Extraction </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsIaKOxrd4h"
      },
      "source": [
        "![](https://i.imgur.com/q1WuWai.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4KwWt67rd4h"
      },
      "source": [
        "# Code: Compound Term Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pn3S4RwHrd4h"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import                               #complete this\n",
        "\n",
        "my_text = \"You all are the greatest students of all time.\"\n",
        "\n",
        "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
        "\n",
        "mwe_tokens\n",
        "\n",
        "# New York City, take into account, make use of, high probability, kick the bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44VAWC7orn-O"
      },
      "source": [
        "# Lambda Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbPyyQ_7rd4u"
      },
      "outputs": [],
      "source": [
        "# Basic example, https://www.w3schools.com/python/python_lambda.asp\n",
        "square_me=lambda x: x*x\n",
        "\n",
        "my_numbers=[9, 3, 4, 100, 2, 1]\n",
        "my_numbers_squared = list(map(square_me, my_numbers)) #map = applies a function to all the items in an input_list\n",
        "                                                      #map(function, iterable)\n",
        "print(my_numbers_squared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1rrNgqErd4u"
      },
      "source": [
        "# <font color=red>Preprocessing Exercise </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G37XzEBLrd4u"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We will be using review data from Kaggle to practice preprocessing text data. The dataset contains user reviews for many products, but today we'll be focusing on the product in the dataset that had the most reviews - an oatmeal cookie.\n",
        "\n",
        "The following code will help you load in the data. If this is your first time using nltk, you'll to need to pip install it first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN4M0O6krd4v"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download() <-- Run this if it's your first time using nltk to download all of the datasets and models\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7wE2LEHrd4v"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('cookie_reviews.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-TQNFEqrd4w"
      },
      "source": [
        "**Question 1:**\n",
        "\n",
        "Determine how many reviews there are in total.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VNVnbS1rd4w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2SHURS_rd4x"
      },
      "source": [
        "**Question 2:**\n",
        "    \n",
        "Determine the percentage of 1, 2, 3, 4 and 5 star reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mrh7DCk4rd4y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHZ7MVt-rd4y"
      },
      "source": [
        "**Question 3:**\n",
        "\n",
        "(a) Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beCkKopYrd4z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHXKLaHfrn-P"
      },
      "source": [
        "1. df['reviews'] refers to the 'reviews' column in your DataFrame df\n",
        "2. .apply(lambda x: ...) is used to apply a function (defined by the lambda function) along the axis of the DataFrame.\n",
        "3. lambda x: ' '.join([word for word in x.split() if word not in (stop)]) is a lambda function that:\n",
        "   <br>a. Splits each review x into a list of words (x.split()).\n",
        "   <br>b. Iterates through each word in this list (for word in x.split()).\n",
        "   <br>c. Checks if each word is not in the stop list (i.e., if it's not a stopword).\n",
        "   <br>d. If the word is not a stopword, it includes it in the list comprehension ([word for word in x.split() if word not in (stop)]).\n",
        "   <br>e. Joins these words back into a single string with spaces separating them (' '.join(...))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr4R-PKWrd4z"
      },
      "source": [
        "(b) Change to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LopdQ216rd40"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00e5zCM5rd40"
      },
      "source": [
        "(b) Perform stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h_Oeeuwrd41"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNEsf7zDrn-Q"
      },
      "source": [
        "1. Constructs a new list (documents) by iterating over each element (x) in the list l_case.\n",
        "2. For each document i in l_case, the inner list comprehension splits i into words using i.split(\" \").\n",
        "3. It then applies stemming to each word using sno.stem(word), where sno is an object or function that performs stemming.\n",
        "4. The outer comprehension gathers these lists of stemmed words (one list per document) and constructs a new list (documents) where each element corresponds to a document from l_case, but with each word stemmed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKTiJ341rd43"
      },
      "source": [
        "# TextBlob\n",
        "\n",
        "### Another toolkit other than NLTK\n",
        "\n",
        "- Wraps around NLTK and makes it easier to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtSKm8Jmrd44"
      },
      "source": [
        "# TextBlob Demo: Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VANPFYO2rn-Q"
      },
      "outputs": [],
      "source": [
        "#pip install textblob  #Install the library before importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sm_gcqLrd44"
      },
      "outputs": [],
      "source": [
        "from textblob import                   #complete this\n",
        "\n",
        "my_text = TextBlob(\"We're moving from NLTK to TextBlob. How fun!\")\n",
        "my_text.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz6kz6o6rd44"
      },
      "source": [
        "# TextBlob Demo: Spell Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7x_TFgdrd45"
      },
      "outputs": [],
      "source": [
        "blob = TextBlob(\"I'm graat at speling.\")\n",
        "print(blob.)                                   #complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvfqZ_ulrd45"
      },
      "source": [
        "<font color=\"blue\">\n",
        "## How does the correct function work?  <br>\n",
        "    \n",
        "- Calculates the Levenshtein distance between the word ‘graat’ and all words in its word list </br>\n",
        "- Of the words with the smallest Levenshtein distance, it outputs the most popular word </br></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNRUkzwyrd46"
      },
      "source": [
        "# TextBlob Demo: Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVr6l2Bkrd46"
      },
      "outputs": [],
      "source": [
        "blob = TextBlob(\"John hits the ball.\")\n",
        "for words, tag in :                           #complete this\n",
        "    print (words, tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRWBpLVfrd46"
      },
      "source": [
        "# TextBlob Demo: Language Detection and Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uimeap4prd47"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"This is a sample text in English.\"\n",
        "blob = TextBlob(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-9hqISZrd47"
      },
      "outputs": [],
      "source": [
        "!pip install                                  #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j9k5_p1rd47"
      },
      "outputs": [],
      "source": [
        "from langdetect import                       #complete this\n",
        "\n",
        "text = \"This is a sample text in English.\"\n",
        "language = detect(text)\n",
        "\n",
        "print(\"Detected Language:\", language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIMWwVwxrn-T"
      },
      "outputs": [],
      "source": [
        "!pip install                        #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wLwTF_trd5g"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "text = \"This is a sample text in English.\"\n",
        "\n",
        "# Detect the language\n",
        "detected_lang = detect(text)\n",
        "\n",
        "# Translate to French\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(text, src=detected_lang, dest='fr').text             #try different language\n",
        "\n",
        "print(\"Detected Language:\", detected_lang)\n",
        "print(\"Translated Text (to French):\", translated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XADY612Arn-T"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxs6mO58rn-T"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using TextBlob to tokenize a given sentence and count the number of tokens.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY5hSejQrn-T"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using TextBlob to perform Parts of Speech (POS) tagging on a given sentence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doy_8CtErn-T"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using TextBlob to perform spell checking on a given text and suggest corrections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slYJnfVQrn-U"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using langdetect and googletrans to perform trasnlation on a given text from english to chiense\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiushqSmrd41"
      },
      "source": [
        "# <font color=\"maroon\"> Some other functions in NLP: Text Similarity Measures </font>\n",
        "\n",
        "- To measure distance between 2 string\n",
        "\n",
        "Applications\n",
        "- Information retrieval\n",
        "- Text classification\n",
        "- Document clustering\n",
        "- Topic Modeling\n",
        "- Matric decomposition\n",
        "\n",
        "To measure the word similarity, we use **<font color=\"blue\"><a href=\"https://pypi.org/project/python-Levenshtein/\" target=\"_blank\">Levenshtein distance</a></font>**.\n",
        "- Minimum number of operations to get from one word to another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHCcSiv3rd41"
      },
      "source": [
        "![](https://i.imgur.com/FkdJmPi.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bvYNWyGrd42"
      },
      "outputs": [],
      "source": [
        "pip install                   #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MlCBsGsrd42"
      },
      "outputs": [],
      "source": [
        "from Levenshtein import distance as lev\n",
        "lev('party', 'park')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svFS8eNIrd43"
      },
      "outputs": [],
      "source": [
        "#concept behind lev('party', 'park')\n",
        "def levenshtein_distance(s1, s2):\n",
        "    m, n = len(s1), len(s2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
        "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
        "\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "string1 = \"party\"\n",
        "string2 = \"park\"\n",
        "distance =                                        #complete this\n",
        "print(\"Levenshtein distance:\", distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-j8ZtXrn-U"
      },
      "source": [
        "## Let's use the Levenshtein to measure the similarity between 2 sentences:\n",
        "<br>sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "<br>sentence2 = \"A quick brown fox jumps over a lazy dog.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJ8pdusrn-U"
      },
      "outputs": [],
      "source": [
        "from Levenshtein import distance as lev\n",
        "\n",
        "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence2 = \"A quick brown fox jumps over a lazy dog.\"\n",
        "\n",
        "words1 = sentence1.lower().split()\n",
        "words2 = sentence2.lower().split()\n",
        "\n",
        "distance = lev(words1, words2)\n",
        "\n",
        "# Calculate similarity (adjust based on your specific needs)\n",
        "max_length = max(len(words1), len(words2))\n",
        "# print (max_length)\n",
        "similarity = 1 - (distance / max_length)\n",
        "\n",
        "print(\"Levenshtein distance between sentence 1 and sentence 2:\", distance)\n",
        "print(\"Similarity between sentence 1 and sentence 2:\", similarity)\n",
        "\n",
        "# However, it's important to note that Levenshtein distance is typically used for comparing sequences of characters, not entire sentences or phrases.\n",
        "# To measure similarity between sentences where the words are not necessarily in the same sequence,\n",
        "# you need to consider methods that can account for semantic similarity rather than just sequence-based similarity like Levenshtein distance.\n",
        "# Here are a few approaches you can explore: TF-IDF/Word Embeddings (pretrained model like Word2Vec, GloVe, or FastText) and Similarity Metrics (Cosine Similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk2GVCZ-rd5g"
      },
      "source": [
        "# Text Format for Analysis: Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD1MS75vrd5g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus =['This is the first document.', 'This is the second document.', 'And the third one. One is fun.'] #corpus=collection of teks\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus)\n",
        "pd.DataFrame(X.toarray(),columns=cv.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdvU8fEVrd5h"
      },
      "source": [
        "![](https://i.imgur.com/OQDeQlb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv94wmmjrd5h"
      },
      "source": [
        "# Document Similarity: Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYKH_Ntyrd5i"
      },
      "source": [
        "![](https://i.imgur.com/PyirXsy.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLZ-ub-Yrd5j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = ['The weather is hot under the sun',\n",
        "'I make my hot chocolate with milk',\n",
        "'One hot encoding',\n",
        "'I will have a chai latte with milk',\n",
        "'There is a hot sale today']\n",
        "# create the document-term matrix with count vectorizer\n",
        "cv = CountVectorizer(stop_words=\"english\")\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zzehizvrd5l"
      },
      "source": [
        "# Document Similarity: Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwo3s5KIrd5l"
      },
      "outputs": [],
      "source": [
        "# calculate the cosine similarity between all combinations of documents\n",
        "from itertools import combinations\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# list all of the combinations of 5 take 2 as well as the pairs of phrases\n",
        "pairs = list(combinations(range(len(corpus)),2)) #sentence (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), .., (3,4))\n",
        "print(pairs)\n",
        "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
        "print (combos)\n",
        "\n",
        "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
        "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
        "sorted(zip(results, combos), reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C9J5J45rd5m"
      },
      "outputs": [],
      "source": [
        "pairs = list(combinations(range(5),2))\n",
        "pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYV4dhzfrd5m"
      },
      "source": [
        "![](https://i.imgur.com/jrfN6Jj.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS0Stotjrd5m"
      },
      "source": [
        "![](https://i.imgur.com/BI8XP92.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_X6soeOrd5n"
      },
      "source": [
        "![](https://i.imgur.com/3IbfQXT.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaTIJB99rd5n"
      },
      "source": [
        "![](https://i.imgur.com/pnNqzql.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBKAYd0erd5n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "corpus = ['This is the first document.',\n",
        "         'This is the second document.',\n",
        "         'And the third one. One is fun.']\n",
        "# original Count Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "pd.DataFrame(X, columns=cv.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u44cBAImrd5o"
      },
      "outputs": [],
      "source": [
        "# new TF-IDF Vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv_tfidf = TfidfVectorizer()\n",
        "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
        "pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEMme_lQrd5o"
      },
      "source": [
        "![](https://i.imgur.com/xlJibKw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIQbXx_Jrd5p"
      },
      "source": [
        "## Document Similarity: Example with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmQKsUwCrd5p"
      },
      "outputs": [],
      "source": [
        "corpus = ['The weather is hot under the sun',\n",
        "'I make my hot chocolate with milk',\n",
        "'One hot encoding',\n",
        "'I will have a chai latte with milk',\n",
        "'There is a hot sale today']\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# create the document-term matrix with TF-IDF vectorizer\n",
        "cv_tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
        "dt_tfidf = pd.DataFrame(X_tfidf,columns=cv_tfidf.get_feature_names())\n",
        "dt_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV41uOddrd5p"
      },
      "outputs": [],
      "source": [
        "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
        "results_tfidf = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
        "sorted(zip(results_tfidf, combos), reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StpSLAU1rd5q"
      },
      "source": [
        "![](https://i.imgur.com/mj4J60v.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}