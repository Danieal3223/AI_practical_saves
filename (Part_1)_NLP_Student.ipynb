{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danieal3223/REPO-1-/blob/main/(Part_1)_NLP_Student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmvFilQfrd2M"
      },
      "source": [
        "# <font color=\"maroon\"> NLP Toolkits and Preprocessing Techniques </font>\n",
        "Python libraries for natural language processing\n",
        "1. Converting text to a meaningful format for analysis\n",
        "2. Preprocessing and cleaning text\n",
        "\n",
        "Open-Source Libraries<br>\n",
        "1. <font color=\"red\">NLTK<br> </font>\n",
        "2. <font color=\"red\">TextBlob<br></font>\n",
        "3. SpaCy<br>\n",
        "4. GenSim<br>\n",
        "\n",
        "Cloud-Based NLP Services<br>\n",
        "1. IBM Watson<br>\n",
        "2. Google Cloud Natural Language API\n",
        "3. Amazon Comprehend\n",
        "4. Microsoft Azure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaWPMOOxrd2T"
      },
      "source": [
        "## How to Install NLTK?\n",
        "\n",
        "### Method (i) Command Line\n",
        "pip install nltk<br>\n",
        "import nltk<br>\n",
        "nltk.download()\n",
        "\n",
        "### Method (ii) Anaconda Navigator (Environment)\n",
        "![Installation of NLTK library](NLTK.png)\n",
        "\n",
        "### Method (iii) Download Package and Place into Site-package directory\n",
        "Install nltk toolkit from https://sourceforge.net/projects/nltk/<br>\n",
        "![Installation of NLTK library](nltk_package.png)\n",
        "<br>Locate the package into site-package directory <br>\n",
        "(to find the path:<br> import site <br>site.getsitepackages())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AfqK8WmIrd2U",
        "outputId": "240cf051-3d11-4d0b-eabb-4752ec71b73a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/usr/local/lib/python3.11/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/lib/python3.11/dist-packages']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import site\n",
        "site.getsitepackages()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySR0BFTirn-H"
      },
      "source": [
        "# Method 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWANqwFjrn-H",
        "outputId": "cc86561b-8d33-4676-817f-821e68d8cd2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk           #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VDPu2TKnrn-H"
      },
      "outputs": [],
      "source": [
        "import nltk #complete this\n",
        "#nltk.download()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.data.path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1fD-GTDt1s9",
        "outputId": "d343d3cd-596d-47ef-bfeb-e829cd0424ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9fuAB41rd2W"
      },
      "source": [
        "## Sample Text Data\n",
        "\n",
        "Consider this sentence:\n",
        "<br>**Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers) from the\n",
        "store. Should I pick up some black-eyed peas as well?**\n",
        "\n",
        "Text data is messy and unstructured. To analyze this data, we need to preprocess the text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gctL6qcrd2Y"
      },
      "source": [
        "![](https://i.imgur.com/pt5p6Hb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJGrFa0qrd2Y"
      },
      "source": [
        "# Code: Tokenization (Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s_xmMSXArd2Y",
        "outputId": "fa397fd8-61fe-406a-fcef-fc0130bc55c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi', 'Mr.', 'Smith', '!', 'I', 'am', 'going', 'to', 'buy', 'some', 'vegetables', '(', '3', 'tomatoes', 'and', '3', 'cucumbers', ')', 'from', 'the', 'store', '.', 'Should', 'I', 'pick', 'up', 'some', 'black-eyed', 'peas', 'as', 'well', '?']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize #complete this\n",
        "\n",
        "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
        "from the store. Should I pick up some black-eyed peas as well?'''\n",
        "\n",
        "print(word_tokenize(my_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbzU3hrZrd2Z"
      },
      "source": [
        "# Code: Tokenization (Sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vEFjou9Mrd2a",
        "outputId": "f6851e95-98f5-4cc3-9f7d-2106db7e12e1",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hi Mr. Smith!', 'I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\\nfrom the store.', 'Should I pick up some black-eyed peas as well?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize #complete this\n",
        "\n",
        "my_text = '''Hi Mr. Smith! I am going to buy some vegetables (3 tomatoes and 3 cucumbers)\n",
        "from the store. Should I pick up some black-eyed peas as well?'''\n",
        "\n",
        "print(sent_tokenize(my_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOhJjUDlrd2e"
      },
      "source": [
        "![](https://i.imgur.com/3L6x92C.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr8o5eNmrd2e"
      },
      "source": [
        "# Code: Remove Punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IgqeFhzerd2e",
        "outputId": "5c68b1ce-cbdf-4eb6-9d55-57817e1b09b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hi Mr Smith I am going to buy some vegetables 3 tomatoes and 3 cucumbers\\nfrom the store Should I pick up some blackeyed peas as well'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import re #complete this\n",
        "import string\n",
        "\n",
        "#Replace punctuations with a white space\n",
        "s = re.sub('[^\\w\\s]','', my_text)             #complete this\n",
        "s\n",
        "\n",
        "#OR\n",
        "# clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)  # string.punctuation is a string defined in the string module of Python. It contains all the punctuation characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~`.\n",
        "# clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwv4BKQzrd2f"
      },
      "source": [
        "# Code: Make All Text Lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "evdfIo5Ird2g",
        "outputId": "6a7e8562-5d7f-4c06-a35d-bdb161e1ea00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI MR SMITH I AM GOING TO BUY SOME VEGETABLES 3 TOMATOES AND 3 CUCUMBERS\\nFROM THE STORE SHOULD I PICK UP SOME BLACKEYED PEAS AS WELL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "clean_text = s.upper()#complete this\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DEi4_gYrd2h"
      },
      "source": [
        "# Code: Remove Numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AAuSuRNBrd2h",
        "outputId": "8beac6d1-686e-47a2-c5e2-23af8f8fb404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HI MR SMITH I AM GOING TO BUY SOME VEGETABLES  TOMATOES AND  CUCUMBERS\\nFROM THE STORE SHOULD I PICK UP SOME BLACKEYED PEAS AS WELL'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Removes all words containing digits\n",
        "clean_text = re.sub('\\d', '', clean_text)  #complete this\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAyxd4Kgrd2i"
      },
      "source": [
        "# <font color='blue'>Preprocessing: Stop Words</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AI9336Hqrd2i"
      },
      "source": [
        "![](https://i.imgur.com/T5RJXrX.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg_vlZdErd4Z"
      },
      "source": [
        "# Code: Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zwAJ5PK1rd4Z",
        "outputId": "7108eb00-b170-4fe4-b4d3-251a738495d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from nltk.corpus import stopwords #complete this\n",
        "nltk.download('stopwords')\n",
        "set(stopwords.words('english'))              #complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWIOjUCrd4a"
      },
      "source": [
        "# Code: Remove Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKfQc-qfrd4a"
      },
      "source": [
        "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8fAaaw_Zrd4b",
        "outputId": "58213bbe-0c94-4c22-8dc4-b3fc45c60828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
            "\twith 13 stored elements and shape (1, 13)>\n",
            "  Coords\tValues\n",
            "  (0, 5)\t1\n",
            "  (0, 6)\t1\n",
            "  (0, 9)\t1\n",
            "  (0, 4)\t1\n",
            "  (0, 1)\t1\n",
            "  (0, 12)\t1\n",
            "  (0, 11)\t1\n",
            "  (0, 2)\t1\n",
            "  (0, 10)\t1\n",
            "  (0, 8)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 3)\t1\n",
            "  (0, 7)\t1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   black  buy  cucumbers  eyed  going  hi  mr  peas  pick  smith  store  \\\n",
              "0      1    1          1     1      1   1   1     1     1      1      1   \n",
              "\n",
              "   tomatoes  vegetables  \n",
              "0         1           1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2e8884e-03e1-4517-bf0a-5a0fc33c94dd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>black</th>\n",
              "      <th>buy</th>\n",
              "      <th>cucumbers</th>\n",
              "      <th>eyed</th>\n",
              "      <th>going</th>\n",
              "      <th>hi</th>\n",
              "      <th>mr</th>\n",
              "      <th>peas</th>\n",
              "      <th>pick</th>\n",
              "      <th>smith</th>\n",
              "      <th>store</th>\n",
              "      <th>tomatoes</th>\n",
              "      <th>vegetables</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2e8884e-03e1-4517-bf0a-5a0fc33c94dd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e2e8884e-03e1-4517-bf0a-5a0fc33c94dd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e2e8884e-03e1-4517-bf0a-5a0fc33c94dd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# self learn numpy: https://www\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"black\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"buy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cucumbers\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eyed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"going\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"peas\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pick\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"smith\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"store\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tomatoes\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vegetables\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer#complete this\n",
        "import pandas as pd\n",
        "\n",
        "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables \\\n",
        "(3 tomatoes and 3 cucumbers from the store. Should I pick up some black-eyed peas as well?\"]\n",
        "\n",
        "# Incorporate stop words when creating the count vectorizer\n",
        "cv = CountVectorizer(stop_words='english') #complete this\n",
        "X = cv.fit_transform(my_text)                                       #complete this\n",
        "print (X)\n",
        "pd.DataFrame(X.toarray(), columns=cv.get_feature_names_out())\n",
        "\n",
        "# Reference: https://www.geeksforgeeks.org/difference-between-pandas-vs-numpy/\n",
        "# self learn pandas: https://www.w3schools.com/python/pandas/pandas_intro.asp\n",
        "# self learn numpy: https://www.w3schools.com/python/numpy/numpy_intro.asp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgPbENbQrd4b"
      },
      "source": [
        "The process of using CountVectorizer.fit_transform involves the following steps:\n",
        "\n",
        "(1)Tokenization: The text documents are first tokenized, breaking them into individual words or tokens.\n",
        "\n",
        "(2)Vocabulary Building (fit): CountVectorizer builds a vocabulary, which is a dictionary mapping each unique word (or token) in the documents to an integer index.\n",
        "\n",
        "(3)Counting (transform): It then counts the occurrences of each word in each document and stores these counts in a sparse matrix, where rows represent documents, and columns represent the vocabulary words. Each element of the matrix represents the frequency of the corresponding word in the respective document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OMyEkOQrd4c"
      },
      "source": [
        "![](https://i.imgur.com/9qllh8j.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlkukpcRrd4c"
      },
      "source": [
        "# Code: Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "yUzdKeTard4c",
        "outputId": "b3965e85-129b-4c8c-e447-b774b0ddd70f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive:driv\n",
            "drives:driv\n",
            "driver:driv\n",
            "drivers:driv\n",
            "driven:driv\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer#complete this\n",
        "stemmer = LancasterStemmer()#complete this\n",
        "\n",
        "# Try some stems\n",
        "print('drive:{}'.format(stemmer.stem('drive')))\n",
        "print('drives:{}'.format(stemmer.stem('drives')))\n",
        "print('driver:{}'.format(stemmer.stem('driver')))\n",
        "print('drivers:{}'.format(stemmer.stem('drivers')))\n",
        "print('driven:{}'.format(stemmer.stem('driven')))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer#complete this\n",
        "stemmer = PorterStemmer()#complete this\n",
        "\n",
        "# Try some stems\n",
        "print('drive:{}'.format(stemmer.stem('drive')))\n",
        "print('drives:{}'.format(stemmer.stem('drives')))\n",
        "print('driver:{}'.format(stemmer.stem('driver')))\n",
        "print('drivers:{}'.format(stemmer.stem('drivers')))\n",
        "print('driven:{}'.format(stemmer.stem('driven')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UTl7zKczVqX",
        "outputId": "bdb29d2c-d87d-4831-a5c9-fa8af909ee80"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive:drive\n",
            "drives:drive\n",
            "driver:driver\n",
            "drivers:driver\n",
            "driven:driven\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvJnHx7Krd4c"
      },
      "source": [
        "# Code: Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zMCMR9c-rd4d",
        "outputId": "e5b50628-c0f0-4222-edeb-c71a4cb86763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "been\n",
            "had\n",
            "done\n",
            "language\n",
            "city\n",
            "mouse\n",
            "running\n",
            "fly\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer#complete this  # Reference: https://www.nltk.org/api/nltk.stem.wordnet.html\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "input_str = \"been had done languages cities mice running flies\"\n",
        "input_str = word_tokenize(input_str)\n",
        "for word in input_str:\n",
        "    print(lemmatizer.lemmatize(word))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkUcOHgBrn-L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kvAH-Lrd4d"
      },
      "source": [
        "![](https://i.imgur.com/8edVsCR.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHDjq6x_rd4e"
      },
      "source": [
        "# Code: Parts of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU1uQFs1rd4e",
        "outputId": "b963f0dc-2e25-42df-890e-999b147b0370"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: [('James', 'NNP'), ('Smith', 'NNP'), ('lives', 'VBZ'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n",
            "Sentence 2: [('James', 'NNP'), ('Smith', 'NNP'), ('is', 'VBZ'), ('having', 'VBG'), ('a', 'DT'), ('live', 'JJ'), ('band', 'NN'), ('in', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tag import pos_tag#complete this\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "my_text2 = \"James Smith is having a live band in the United States.\"\n",
        "tokens = pos_tag(word_tokenize(my_text))\n",
        "tokens2 = pos_tag(word_tokenize(my_text2))\n",
        "print(\"Sentence 1:\",tokens)\n",
        "print(\"Sentence 2:\",tokens2)\n",
        "\n",
        "#Reference:https://pythonspot.com/nltk-speech-tagging/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vfPiB0Erd4f"
      },
      "source": [
        "![POS](nltk-speech-codes.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfpmNQBHrd4f"
      },
      "source": [
        "## Named Entity Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "-clqK5cCrd4f",
        "outputId": "7076c31e-f786-4717-c9af-dfa71fb2587c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-2322008140.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this labels each word as a part of speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this extracts entities from the list of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ],
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "from nltk.chunk import ne_chunk #complete this\n",
        "\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
        "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
        "entities.draw()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk#complete this\n",
        "\n",
        "#download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "\n",
        "my_text = \"James Smith lives in the United States.\"\n",
        "\n",
        "tokens = word_tokenize(my_text) # this labels each word as a part of speech\n",
        "tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "entities = ne_chunk(tagged_tokens)\n",
        "\n",
        "entities.draw()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "G9EnEKuH1-WD",
        "outputId": "ca46cf88-5215-4e41-897a-063a6b6c89fc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TclError",
          "evalue": "no display name and no $DISPLAY environment variable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-33-3247453708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tree/tree.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraw_trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         \u001b[0mdraw_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpretty_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhighlight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36mdraw_trees\u001b[0;34m(*trees)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m     \"\"\"\n\u001b[0;32m-> 1008\u001b[0;31m     \u001b[0mTreeView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrees\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/draw/tree.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *trees)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_top\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<Control-x>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjiixNgFrd4f"
      },
      "source": [
        "# <font color=\"blue\"> Prepocessing: Compound Term Extraction </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXsIaKOxrd4h"
      },
      "source": [
        "![](https://i.imgur.com/q1WuWai.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4KwWt67rd4h"
      },
      "source": [
        "# Code: Compound Term Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pn3S4RwHrd4h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c241250-0acf-4c48-d4eb-267b4d4414bd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['You_all', 'are', 'the', 'greatest', 'students', 'of_all_time', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import MWETokenizer #complete this\n",
        "\n",
        "my_text = \"You all are the greatest students of all time.\"\n",
        "\n",
        "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
        "\n",
        "mwe_tokens\n",
        "\n",
        "# New York City, take into account, make use of, high probability, kick the bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44VAWC7orn-O"
      },
      "source": [
        "# Lambda Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hbPyyQ_7rd4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b1de05-4aae-41b6-9664-e18deacdd114"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[81, 9, 16, 10000, 4, 1]\n"
          ]
        }
      ],
      "source": [
        "# Basic example, https://www.w3schools.com/python/python_lambda.asp\n",
        "square_me=lambda x: x*x\n",
        "\n",
        "my_numbers=[9, 3, 4, 100, 2, 1]\n",
        "my_numbers_squared = list(map(square_me, my_numbers)) #map = applies a function to all the items in an input_list\n",
        "                                                      #map(function, iterable)\n",
        "print(my_numbers_squared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1rrNgqErd4u"
      },
      "source": [
        "# <font color=red>Preprocessing Exercise </font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G37XzEBLrd4u"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We will be using review data from Kaggle to practice preprocessing text data. The dataset contains user reviews for many products, but today we'll be focusing on the product in the dataset that had the most reviews - an oatmeal cookie.\n",
        "\n",
        "The following code will help you load in the data. If this is your first time using nltk, you'll to need to pip install it first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UN4M0O6krd4v"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download() <-- Run this if it's your first time using nltk to download all of the datasets and models\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-7wE2LEHrd4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "1d8838f6-6dfa-4dbb-aa87-35761f8e12be"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            user_id  stars                                            reviews\n",
              "813  A1JBOM5L972U4P      5  I was choosen as one of @Influensters Mom VoxB...\n",
              "814   ANEIQUS6I8RJ5      4  This cookie is good the only thing I had a pro...\n",
              "815  A221B991YB1LXU      5  This cookie was soft and full of flavor. I fel...\n",
              "816  A1ZLP5VN8OLTQA      5  I received a free sample from Influenster, and...\n",
              "817  A2BYNLAM76HR2H      5  This is an awesomely soft cookie. I loved this...\n",
              "..              ...    ...                                                ...\n",
              "908  A366PSH7KFLRPB      5  I loved these cookies and so did my kids. You ...\n",
              "909  A2KV6EYQPKJRR5      5  This is a great tasting cookie. It is very sof...\n",
              "910  A3O7REI0OSV89M      4  These are great for a quick snack! They are sa...\n",
              "911   A9JS5GQQ6GIQT      5  I love the Quaker soft baked cookies.  The rea...\n",
              "912   AMAVEZAGCH52H      5  This cookie is really good and works really we...\n",
              "\n",
              "[100 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-24a61398-9851-46f2-90eb-bdee061073d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>813</th>\n",
              "      <td>A1JBOM5L972U4P</td>\n",
              "      <td>5</td>\n",
              "      <td>I was choosen as one of @Influensters Mom VoxB...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>ANEIQUS6I8RJ5</td>\n",
              "      <td>4</td>\n",
              "      <td>This cookie is good the only thing I had a pro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>815</th>\n",
              "      <td>A221B991YB1LXU</td>\n",
              "      <td>5</td>\n",
              "      <td>This cookie was soft and full of flavor. I fel...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>816</th>\n",
              "      <td>A1ZLP5VN8OLTQA</td>\n",
              "      <td>5</td>\n",
              "      <td>I received a free sample from Influenster, and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>817</th>\n",
              "      <td>A2BYNLAM76HR2H</td>\n",
              "      <td>5</td>\n",
              "      <td>This is an awesomely soft cookie. I loved this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>908</th>\n",
              "      <td>A366PSH7KFLRPB</td>\n",
              "      <td>5</td>\n",
              "      <td>I loved these cookies and so did my kids. You ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>909</th>\n",
              "      <td>A2KV6EYQPKJRR5</td>\n",
              "      <td>5</td>\n",
              "      <td>This is a great tasting cookie. It is very sof...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>910</th>\n",
              "      <td>A3O7REI0OSV89M</td>\n",
              "      <td>4</td>\n",
              "      <td>These are great for a quick snack! They are sa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>911</th>\n",
              "      <td>A9JS5GQQ6GIQT</td>\n",
              "      <td>5</td>\n",
              "      <td>I love the Quaker soft baked cookies.  The rea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>912</th>\n",
              "      <td>AMAVEZAGCH52H</td>\n",
              "      <td>5</td>\n",
              "      <td>This cookie is really good and works really we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-24a61398-9851-46f2-90eb-bdee061073d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-24a61398-9851-46f2-90eb-bdee061073d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-24a61398-9851-46f2-90eb-bdee061073d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-3b97608f-7644-4c46-9cd3-aa00bc94a24e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3b97608f-7644-4c46-9cd3-aa00bc94a24e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-3b97608f-7644-4c46-9cd3-aa00bc94a24e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"user_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"A1QA81DFWWGPPN\",\n          \"A3CKZNWP2ILJP\",\n          \"A2BEW6FNOW66XX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stars\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 5,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reviews\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 99,\n        \"samples\": [\n          \"Let me start off by saying I'm a bit of a cookie snob having lived next door to my grandmother for most of my youth.  There's nothing better than a homemade cookie.  Having tried these and shared them with some co-workers I can honestly say that it's not just me that wasn't impressed.  There wasn't much said that was positive.  For being a soft baked cookie it tasted awfully dry.  It was almost more like a cereal bar rather than a cookie.  There was no yummy buttery taste.  They just weren't worth the 170 cals per cookie.  The only thing positive that was said is that it is convienient to have them separately packed for lunches and on the go snacks.\",\n          \"The Quaker Brand soft cookies are so good! They are yummy and soft and really fill you up! So easy on the go with the kids or if your just relaxing at home with some milk!\",\n          \"This is a great tasting cookie. It is very soft and the texture is very smooth. I usually do not like oatmeal cookies because the texture is very gritty and tough. This cookie does not have that problem. It tastes like your Mom just made it.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "df = pd.read_csv('cookie_reviews.csv')\n",
        "df.head(10)\n",
        "df.iloc[5:15]\n",
        "df.tail(100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('cookie_reviews.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3UgnePgY4vTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-TQNFEqrd4w"
      },
      "source": [
        "**Question 1:**\n",
        "\n",
        "Determine how many reviews there are in total.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "8VNVnbS1rd4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9399a29-fa15-430c-a7d6-b01530d3512a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "913\n",
            "user_id    913\n",
            "stars      913\n",
            "reviews    913\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('cookie_reviews.csv')\n",
        "total_review = len(df)\n",
        "print(total_review)\n",
        "\n",
        "#ans\n",
        "total = df.count()\n",
        "print(total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2SHURS_rd4x"
      },
      "source": [
        "**Question 2:**\n",
        "    \n",
        "Determine the percentage of 1, 2, 3, 4 and 5 star reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "Mrh7DCk4rd4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "953c6fc4-5411-4a04-d3d5-96ebfbe96717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stars: 4\n",
            "stars: 12\n",
            "stars: 56\n",
            "stars: 217\n",
            "stars: 624\n",
            "stars\n",
            "5    624\n",
            "4    217\n",
            "3     56\n",
            "2     12\n",
            "1      4\n",
            "Name: count, dtype: int64\n",
            "stars\n",
            "5    68.346112\n",
            "4    23.767798\n",
            "3     6.133625\n",
            "2     1.314348\n",
            "1     0.438116\n",
            "Name: count, dtype: float64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-55-1850324567.py:19: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  print(val/total[1]*100)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('cookie_reviews.csv')\n",
        "stars = df['stars'].tolist()\n",
        "star1 = stars.count(1)\n",
        "star2 = stars.count(2)\n",
        "star3 = stars.count(3)\n",
        "star4 = stars.count(4)\n",
        "star5 = stars.count(5)\n",
        "print(\"1 stars:\", star1)\n",
        "print(\"2 stars:\", star2)\n",
        "print(\"3 stars:\", star3)\n",
        "print(\"4 stars:\", star4)\n",
        "print(\"5 stars:\", star5)\n",
        "\n",
        "#ans\n",
        "val=df['stars'].value_counts()\n",
        "print(val)\n",
        "\n",
        "print(val/total[1]*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHZ7MVt-rd4y"
      },
      "source": [
        "**Question 3:**\n",
        "\n",
        "(a) Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beCkKopYrd4z"
      },
      "outputs": [],
      "source": [
        "# Basic example, https://www.w3schools.com/python/python_lambda.asp\n",
        "import pandas as pd\n",
        "df = pd.read_csv('cookie_reviews.csv')\n",
        "stars = df['reviews'].tolist()\n",
        "square_me=lambda x: x*x\n",
        "\n",
        "my_numbers=[9, 3, 4, 100, 2, 1]\n",
        "my_numbers_squared = list(map(square_me, my_numbers)) #map = applies a function to all the items in an input_list\n",
        "                                                      #map(function, iterable)\n",
        "print(my_numbers_squared)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHXKLaHfrn-P"
      },
      "source": [
        "1. df['reviews'] refers to the 'reviews' column in your DataFrame df\n",
        "2. .apply(lambda x: ...) is used to apply a function (defined by the lambda function) along the axis of the DataFrame.\n",
        "3. lambda x: ' '.join([word for word in x.split() if word not in (stop)]) is a lambda function that:\n",
        "   <br>a. Splits each review x into a list of words (x.split()).\n",
        "   <br>b. Iterates through each word in this list (for word in x.split()).\n",
        "   <br>c. Checks if each word is not in the stop list (i.e., if it's not a stopword).\n",
        "   <br>d. If the word is not a stopword, it includes it in the list comprehension ([word for word in x.split() if word not in (stop)]).\n",
        "   <br>e. Joins these words back into a single string with spaces separating them (' '.join(...))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr4R-PKWrd4z"
      },
      "source": [
        "(b) Change to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LopdQ216rd40"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00e5zCM5rd40"
      },
      "source": [
        "(b) Perform stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h_Oeeuwrd41"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNEsf7zDrn-Q"
      },
      "source": [
        "1. Constructs a new list (documents) by iterating over each element (x) in the list l_case.\n",
        "2. For each document i in l_case, the inner list comprehension splits i into words using i.split(\" \").\n",
        "3. It then applies stemming to each word using sno.stem(word), where sno is an object or function that performs stemming.\n",
        "4. The outer comprehension gathers these lists of stemmed words (one list per document) and constructs a new list (documents) where each element corresponds to a document from l_case, but with each word stemmed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKTiJ341rd43"
      },
      "source": [
        "# TextBlob\n",
        "\n",
        "### Another toolkit other than NLTK\n",
        "\n",
        "- Wraps around NLTK and makes it easier to use"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtSKm8Jmrd44"
      },
      "source": [
        "# TextBlob Demo: Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VANPFYO2rn-Q",
        "outputId": "672b28dc-84f4-458a-cf65-8529e117fbc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: TextBlob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from TextBlob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->TextBlob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->TextBlob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->TextBlob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->TextBlob) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "#pip install textblob  #Install the library before importing\n",
        "!pip install TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sm_gcqLrd44"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob#complete this\n",
        "\n",
        "my_text = TextBlob(\"We're moving from NLTK to TextBlob. How fun!\")\n",
        "my_text.words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vz6kz6o6rd44"
      },
      "source": [
        "# TextBlob Demo: Spell Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "a7x_TFgdrd45",
        "outputId": "7f1021dd-23e1-4168-ee5e-2434125556ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TextBlob' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-62-4244857443.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I'm graat at speling.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m                                   \u001b[0;31m#complete this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TextBlob' is not defined"
          ]
        }
      ],
      "source": [
        "blob = TextBlob(\"I'm graat at speling.\")\n",
        "print(blob.correct())                                   #complete this"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvfqZ_ulrd45"
      },
      "source": [
        "<font color=\"blue\">\n",
        "## How does the correct function work?  <br>\n",
        "    \n",
        "- Calculates the Levenshtein distance between the word ‘graat’ and all words in its word list </br>\n",
        "- Of the words with the smallest Levenshtein distance, it outputs the most popular word </br></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNRUkzwyrd46"
      },
      "source": [
        "# TextBlob Demo: Tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVr6l2Bkrd46"
      },
      "outputs": [],
      "source": [
        "blob = TextBlob(\"John hits the ball.\")\n",
        "for words, tag in :                           #complete this\n",
        "    print (words, tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRWBpLVfrd46"
      },
      "source": [
        "# TextBlob Demo: Language Detection and Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "Uimeap4prd47",
        "outputId": "333d70b3-6022-4f5d-b771-9565c4a17680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'language' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-64-894794381.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Detech Language: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'language' is not defined"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"This is a sample text in English.\"\n",
        "blob = TextBlob(text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "W-9hqISZrd47",
        "outputId": "1019a7eb-31a7-4286-a016-2c4559efa690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/981.5 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m972.8/981.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=ddbc4b46c5a77df856ed180abeef3c6b6d1a9c449dc91e5c075340929429ebf4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "6j9k5_p1rd47",
        "outputId": "84e52ea2-cd3a-4b81-891c-b059f69182b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Language: en\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect#complete this\n",
        "\n",
        "text = \"This is a sample text in English.\"\n",
        "language = detect(text)\n",
        "\n",
        "print(\"Detected Language:\", language)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZIMWwVwxrn-T",
        "outputId": "df8f8232-d45c-4bec-8772-2ce826434628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.6.15)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=468ea836e079ee96b7f15420dcd75ca5ca3bb33c9687d1c5b46bc7c1f4f9a610\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.23.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.4.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.93.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio 5.31.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 1.10.1 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              },
              "id": "ba7900e8826843739bb5938163ba67f1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install googletrans==4.0.0-rc1 #complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "2wLwTF_trd5g",
        "outputId": "4b1a6c95-7b55-4190-a4f5-6acf0eba1e3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your sentence here > My name is not your concern\n",
            "Enter your to translate to language here (en = english, zh-tw = chinese, ja = japanese) > ja\n",
            "Detected Language: en\n",
            "Translated Text (to French): これは英語のサンプルテキストです。\n"
          ]
        }
      ],
      "source": [
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "\n",
        "#text = \"This is a sample text in English.\"\n",
        "user_input = input(\"Enter your sentence here > \")\n",
        "\n",
        "# Detect the language\n",
        "detected_lang = detect(user_input)\n",
        "\n",
        "user_transLan = input(\"Enter your to translate to language here (en = english, zh-tw = chinese, ja = japanese) > \")\n",
        "# Translate to French\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(text, src=detected_lang, dest=user_transLan).text             #try different language\n",
        "\n",
        "print(\"Detected Language:\", detected_lang)\n",
        "print(\"Translated Text (to French):\", translated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XADY612Arn-T"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxs6mO58rn-T",
        "outputId": "960ba971-5286-4047-d60a-ad19490db073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['TextBlob', 'is', 'a', 'simple', 'and', 'powerful', 'library', 'for', 'text', 'processing']\n",
            "Number of tokens: 10\n"
          ]
        }
      ],
      "source": [
        "# Write a Python function using TextBlob to tokenize a given sentence and count the number of tokens.\n",
        "#ans:\n",
        "from textblob import TextBlob\n",
        "\n",
        "def tokenize_and_count(sentence):\n",
        "    blob = TextBlob(sentence)\n",
        "    tokens = blob.words\n",
        "    print(\"Tokens:\", tokens)\n",
        "    num_tokens = len(tokens)\n",
        "    return num_tokens\n",
        "\n",
        "sentence = \"TextBlob is a simple and powerful library for text processing\"\n",
        "\n",
        "num_tokens = tokenize_and_count(sentence)\n",
        "print(\"Number of tokens:\", num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY5hSejQrn-T",
        "outputId": "87edf0e6-5ddb-47ac-f0b5-7243d6e284d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextBlob NNP\n",
            "is VBZ\n",
            "a DT\n",
            "simple JJ\n",
            "and CC\n",
            "powerful JJ\n",
            "library NN\n",
            "for IN\n",
            "text NN\n",
            "processing NN\n"
          ]
        }
      ],
      "source": [
        "# Write a Python function using TextBlob to perform Parts of Speech (POS) tagging on a given sentence.\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "def pos_tagging(sentence):\n",
        "    blob = TextBlob(sentence)\n",
        "    pos_tags = blob.tags\n",
        "    for words, tag in blob.tags:\n",
        "      print (words, tag)\n",
        "\n",
        "sentence = \"TextBlob is a simple and powerful library for text processing\"\n",
        "\n",
        "pos_tagging(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doy_8CtErn-T"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using TextBlob to perform spell checking on a given text and suggest corrections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slYJnfVQrn-U"
      },
      "outputs": [],
      "source": [
        "# Write a Python function using langdetect and googletrans to perform trasnlation on a given text from english to chiense\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiushqSmrd41"
      },
      "source": [
        "# <font color=\"maroon\"> Some other functions in NLP: Text Similarity Measures </font>\n",
        "\n",
        "- To measure distance between 2 string\n",
        "\n",
        "Applications\n",
        "- Information retrieval\n",
        "- Text classification\n",
        "- Document clustering\n",
        "- Topic Modeling\n",
        "- Matric decomposition\n",
        "\n",
        "To measure the word similarity, we use **<font color=\"blue\"><a href=\"https://pypi.org/project/python-Levenshtein/\" target=\"_blank\">Levenshtein distance</a></font>**.\n",
        "- Minimum number of operations to get from one word to another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHCcSiv3rd41"
      },
      "source": [
        "![](https://i.imgur.com/FkdJmPi.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "9bvYNWyGrd42",
        "outputId": "e4b3ea0c-35ea-4412-86f1-071f78ecc007",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'python_implementation#complete': Expected end or semicolon (after name and no valid version specifier)\n",
            "    python_implementation#complete\n",
            "                         ^\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "from platform import python_implementation\n",
        "!pip install python_implementation#complete this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MlCBsGsrd42"
      },
      "outputs": [],
      "source": [
        "from Levenshtein import distance as lev\n",
        "lev('party', 'park')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svFS8eNIrd43"
      },
      "outputs": [],
      "source": [
        "#concept behind lev('party', 'park')\n",
        "def levenshtein_distance(s1, s2):\n",
        "    m, n = len(s1), len(s2)\n",
        "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "    for i in range(m + 1):\n",
        "        dp[i][0] = i\n",
        "\n",
        "    for j in range(n + 1):\n",
        "        dp[0][j] = j\n",
        "\n",
        "    for i in range(1, m + 1):\n",
        "        for j in range(1, n + 1):\n",
        "            cost = 0 if s1[i - 1] == s2[j - 1] else 1\n",
        "            dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
        "\n",
        "    return dp[m][n]\n",
        "\n",
        "# Example usage\n",
        "string1 = \"party\"\n",
        "string2 = \"park\"\n",
        "distance =                                        #complete this\n",
        "print(\"Levenshtein distance:\", distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v-j8ZtXrn-U"
      },
      "source": [
        "## Let's use the Levenshtein to measure the similarity between 2 sentences:\n",
        "<br>sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "<br>sentence2 = \"A quick brown fox jumps over a lazy dog.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJ8pdusrn-U"
      },
      "outputs": [],
      "source": [
        "from Levenshtein import distance as lev\n",
        "\n",
        "sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "sentence2 = \"A quick brown fox jumps over a lazy dog.\"\n",
        "\n",
        "words1 = sentence1.lower().split()\n",
        "words2 = sentence2.lower().split()\n",
        "\n",
        "distance = lev(words1, words2)\n",
        "\n",
        "# Calculate similarity (adjust based on your specific needs)\n",
        "max_length = max(len(words1), len(words2))\n",
        "# print (max_length)\n",
        "similarity = 1 - (distance / max_length)\n",
        "\n",
        "print(\"Levenshtein distance between sentence 1 and sentence 2:\", distance)\n",
        "print(\"Similarity between sentence 1 and sentence 2:\", similarity)\n",
        "\n",
        "# However, it's important to note that Levenshtein distance is typically used for comparing sequences of characters, not entire sentences or phrases.\n",
        "# To measure similarity between sentences where the words are not necessarily in the same sequence,\n",
        "# you need to consider methods that can account for semantic similarity rather than just sequence-based similarity like Levenshtein distance.\n",
        "# Here are a few approaches you can explore: TF-IDF/Word Embeddings (pretrained model like Word2Vec, GloVe, or FastText) and Similarity Metrics (Cosine Similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk2GVCZ-rd5g"
      },
      "source": [
        "# Text Format for Analysis: Count Vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sD1MS75vrd5g"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus =['This is the first document.', 'This is the second document.', 'And the third one. One is fun.'] #corpus=collection of teks\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus)\n",
        "pd.DataFrame(X.toarray(),columns=cv.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdvU8fEVrd5h"
      },
      "source": [
        "![](https://i.imgur.com/OQDeQlb.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv94wmmjrd5h"
      },
      "source": [
        "# Document Similarity: Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYKH_Ntyrd5i"
      },
      "source": [
        "![](https://i.imgur.com/PyirXsy.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLZ-ub-Yrd5j"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = ['The weather is hot under the sun',\n",
        "'I make my hot chocolate with milk',\n",
        "'One hot encoding',\n",
        "'I will have a chai latte with milk',\n",
        "'There is a hot sale today']\n",
        "# create the document-term matrix with count vectorizer\n",
        "cv = CountVectorizer(stop_words=\"english\")\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
        "dt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zzehizvrd5l"
      },
      "source": [
        "# Document Similarity: Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwo3s5KIrd5l"
      },
      "outputs": [],
      "source": [
        "# calculate the cosine similarity between all combinations of documents\n",
        "from itertools import combinations\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# list all of the combinations of 5 take 2 as well as the pairs of phrases\n",
        "pairs = list(combinations(range(len(corpus)),2)) #sentence (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), .., (3,4))\n",
        "print(pairs)\n",
        "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
        "print (combos)\n",
        "\n",
        "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
        "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
        "sorted(zip(results, combos), reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C9J5J45rd5m"
      },
      "outputs": [],
      "source": [
        "pairs = list(combinations(range(5),2))\n",
        "pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYV4dhzfrd5m"
      },
      "source": [
        "![](https://i.imgur.com/jrfN6Jj.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS0Stotjrd5m"
      },
      "source": [
        "![](https://i.imgur.com/BI8XP92.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_X6soeOrd5n"
      },
      "source": [
        "![](https://i.imgur.com/3IbfQXT.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaTIJB99rd5n"
      },
      "source": [
        "![](https://i.imgur.com/pnNqzql.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBKAYd0erd5n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "corpus = ['This is the first document.',\n",
        "         'This is the second document.',\n",
        "         'And the third one. One is fun.']\n",
        "# original Count Vectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "pd.DataFrame(X, columns=cv.get_feature_names())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u44cBAImrd5o"
      },
      "outputs": [],
      "source": [
        "# new TF-IDF Vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "cv_tfidf = TfidfVectorizer()\n",
        "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
        "pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEMme_lQrd5o"
      },
      "source": [
        "![](https://i.imgur.com/xlJibKw.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIQbXx_Jrd5p"
      },
      "source": [
        "## Document Similarity: Example with TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmQKsUwCrd5p"
      },
      "outputs": [],
      "source": [
        "corpus = ['The weather is hot under the sun',\n",
        "'I make my hot chocolate with milk',\n",
        "'One hot encoding',\n",
        "'I will have a chai latte with milk',\n",
        "'There is a hot sale today']\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# create the document-term matrix with TF-IDF vectorizer\n",
        "cv_tfidf = TfidfVectorizer(stop_words=\"english\")\n",
        "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
        "dt_tfidf = pd.DataFrame(X_tfidf,columns=cv_tfidf.get_feature_names())\n",
        "dt_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QV41uOddrd5p"
      },
      "outputs": [],
      "source": [
        "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
        "results_tfidf = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
        "sorted(zip(results_tfidf, combos), reverse=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StpSLAU1rd5q"
      },
      "source": [
        "![](https://i.imgur.com/mj4J60v.png)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}